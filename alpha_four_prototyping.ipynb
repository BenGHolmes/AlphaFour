{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from agents import Agent, AlphaFour\n",
    "from collections import namedtuple, deque\n",
    "from random import choice, sample\n",
    "import importlib\n",
    "from connectboard import ConnectBoard\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_game_state(game_board: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the AlphaFour representation of the game board.\n",
    "    \n",
    "    When AlphaFour gets the game board, the current player is 1 \n",
    "    and the opponent is -1. We translate that to a two layer state\n",
    "    where the current player's pieces are 1's in the first slice,\n",
    "    and the opponent's pieces are 1's in the second slice\n",
    "    \n",
    "    Args:\n",
    "        game_board: A numpy array representing the current game state.\n",
    "        \n",
    "    Returns:\n",
    "        A numpy array containing the AlphaFour representation of the \n",
    "        given game state.\n",
    "    \"\"\"\n",
    "    p1_pieces = np.where(game_board == 1, 1, 0)\n",
    "    p2_pieces = np.where(game_board == -1, 1, 0)\n",
    "    alpha_four_state = np.array([p1_pieces, p2_pieces])\n",
    "\n",
    "    return alpha_four_state\n",
    "\n",
    "# Named tuple for training data. Stores state, move probabilities, and state value\n",
    "TrainingSample = namedtuple('TrainingSample', 'state probs value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play(agent1: AlphaFour, agent2: AlphaFour):\n",
    "    game_board = np.zeros((6,7))\n",
    "    turn = 0\n",
    "        \n",
    "    states = []\n",
    "    probs = []\n",
    "    values = []\n",
    "    \n",
    "    state = get_game_state(game_board)\n",
    "    \n",
    "    while True:\n",
    "        states.append(state.copy())  # Add current state\n",
    "\n",
    "        if state[2].all():\n",
    "            # Get best move, and probability of all moves from current state\n",
    "            move,prob = agent1.get_move_with_prob(state)\n",
    "            state[0] += move\n",
    "            state[2] = np.zeros((6,7))\n",
    "        else:\n",
    "            move,prob = agent2.get_move_with_prob(state)\n",
    "            state[1] += move\n",
    "            state[2] = np.ones((6,7))\n",
    "        \n",
    "        probs.append(prob) # Store move probabilities from current state\n",
    "                \n",
    "        val = helpers.winner(state[0] - state[1])\n",
    "        if val is not None:\n",
    "            # Add the final state to our arrays\n",
    "            states.append(state.copy())\n",
    "            probs.append(np.zeros((1,7)))\n",
    "            \n",
    "            val = -1*abs(val) # If game is over, current player lost unless it's a tie. \n",
    "            break\n",
    "        \n",
    "        turn += 1\n",
    "                \n",
    "    for i in range(len(probs)):\n",
    "        values.append(val * (-1)**i)\n",
    "    values = values[::-1]\n",
    "    \n",
    "    data = [Data(states[i], probs[i], values[i]) for i in range(len(probs))]\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benholmes/Documents/Projects/AlphaFour/agents/alphafour.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ucb = np.where(node.N > 0, node.W/node.N + self._EXPLORATION_CONSTANT*node.P/(1+node.N), np.inf)\n"
     ]
    }
   ],
   "source": [
    "D = self_play(AlphaFour('P1'), AlphaFour('P2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c711bacdedd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8736f4ae3f5f>\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(agent1, agent2)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Get best move, and probability of all moves from current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mmove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_move_with_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/AlphaFour/agents/alphafour.py\u001b[0m in \u001b[0;36mget_move_with_prob\u001b[0;34m(self, game_state)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mleaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mwinner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwinner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# If leaf has static value it has no children. Back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Projects/AlphaFour/helpers.py\u001b[0m in \u001b[0;36mwinner\u001b[0;34m(game_board)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"    \n\u001b[1;32m     71\u001b[0m     \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_board\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWINDOW_INDICES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0muncontested_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# If there are any windows with only 1's or -1's, check if any are full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     28\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     29\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "REPLAY_BUFFER_SIZE = 100000  # Number of past steps to store. This is where our training sample is drawn from\n",
    "SELF_PLAY_BATCH_SIZE = 100 # How many games to play before updating the buffer\n",
    "TRAINING_SET_SIZE = 1024    # Size of the training set to sample from the replay buffer\n",
    "\n",
    "replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "\n",
    "# START OF ONE TRAINING LOOP\n",
    "# ==========================================================================\n",
    "players = [AlphaFour('Best'), AlphaFour('New')]\n",
    "\n",
    "total_count = 0\n",
    "\n",
    "# Generate new self play games.\n",
    "for ii in range(SELF_PLAY_BATCH_SIZE):\n",
    "    print(f'\\r{ii}', end='')\n",
    "    i = np.random.randint(2)  # randomize who plays first\n",
    "    p1 = players[i]\n",
    "    p2 = players[(i+1)%2]\n",
    "    \n",
    "    D = self_play(p1,p2)\n",
    "    \n",
    "    for d in D:\n",
    "        total_count += 1\n",
    "        replay_buffer.append(d)\n",
    "        \n",
    "train_set = sample(replay_buffer, TRAINING_SET_SIZE)\n",
    "\n",
    "# Train new bot with updated data\n",
    "\n",
    "# Play matches between new and old. If new wins more than 55%, replace old with New\n",
    "\n",
    "\n",
    "# =========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_set[123].S\n",
    "moves = helpers.get_legal_moves(state[0] + state[1])\n",
    "moves = moves[1:,:,:]\n",
    "print(moves)\n",
    "\n",
    "for col in range(7):\n",
    "    print(moves[:,:,col].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_set[123].S\n",
    "moves = helpers.get_legal_moves(state[0] + state[1])\n",
    "moves = moves[1:]\n",
    "\n",
    "col_has_move = moves.sum(axis=0).sum(axis=0)\n",
    "\n",
    "move_idx = 0\n",
    "for i in range(7):\n",
    "    if col_has_move[i]:\n",
    "        new_state = state + np.array([moves[move_idx], np.zeros((6,7)), np.zeros((6,7))])\n",
    "        move_idx += 1\n",
    "        print(new_state, '\\n\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
